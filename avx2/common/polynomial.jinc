require "arithmetic/modular.jinc"

// For use in fn pointwise_montgomery_multiply_and_reduce() below
u256 MODULUS_VECTOR = (8u32)[
    MODULUS,
    MODULUS,
    MODULUS,
    MODULUS,
    MODULUS,
    MODULUS,
    MODULUS,
    MODULUS
];
u256 INVERSE_OF_MODULUS_MOD_MONTGOMERY_R_VECTOR = (8u32)[
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R,
    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R
];

u256 TWO_POW_22_VECTOR = (8u32)[
    1 << 22,
    1 << 22,
    1 << 22,
    1 << 22,
    1 << 22,
    1 << 22,
    1 << 22,
    1 << 22
];


namespace polynomial {
    fn add(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] lhs_pointer rhs_pointer sum_pointer)
        -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        reg u256 lhs rhs sum;

        reg u64 offset;

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            lhs = lhs_pointer.[u256 offset];
            rhs = rhs_pointer.[u256 offset];

            sum = #VPADD_8u32(lhs, rhs);
            sum_pointer.[u256 offset] = sum;

            offset += 32;
        }

        return sum_pointer;
    }

    fn subtract(
        reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] lhs_pointer rhs_pointer difference_pointer
    )
    -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] {
        reg u256 lhs rhs difference;

        reg u64 offset;

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            lhs = lhs_pointer.[u256 offset];
            rhs = rhs_pointer.[u256 offset];

            difference = #VPSUB_8u32(lhs, rhs);
            difference_pointer.[u256 offset] = difference;

            offset += 32;
        }

        return difference_pointer;
    }

    inline
    fn add_to_running_total(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial running_total)
        -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        reg u256 lhs rhs sum;

        reg u64 offset;

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            lhs = polynomial.[u256 offset];
            rhs = running_total.[u256 offset];

            sum = #VPADD_8u32(lhs, rhs);
            running_total.[u256 offset] = sum;

            offset += 32;
        }

        return running_total;
    }

    inline
    fn zero(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial)
        -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        reg u256 zero_u256;
        reg u64 offset;

        ?{}, zero_u256 = #set0_256();

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            #VMOVDQU_256(polynomial.[u256 offset]) = zero_u256;
            offset += 32;
        }

        return polynomial;
    }

    fn pointwise_montgomery_multiply_and_reduce(
        reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] lhs rhs product
    ) -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        // TODO: If performance is lacking, unroll this loop to process 3 
        // chunks per round.

        reg u256 lhs_low lhs_high rhs_low rhs_high;
        reg u256 product_low product_high;
        reg u256 t_low t_high;

        reg u64 offset;

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            lhs_low = lhs.[u256 offset];
            rhs_low = rhs.[u256 offset];

            lhs_high = #VMOVSHDUP_256(lhs_low);
            rhs_high = #VMOVSHDUP_256(rhs_low);

            // Multiply
            product_low = #VPMUL_256(lhs_low, rhs_low);
            product_high = #VPMUL_256(lhs_high, rhs_high);

            // Reduce
            t_low = #VPMUL_256(product_low,
                    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R_VECTOR);
            t_high = #VPMUL_256(product_high,
                    INVERSE_OF_MODULUS_MOD_MONTGOMERY_R_VECTOR);
            t_low = #VPMUL_256(t_low, MODULUS_VECTOR);
            t_high = #VPMUL_256(t_high, MODULUS_VECTOR);

            product_low = #VPSUB_4u64(product_low, t_low);
            product_high = #VPSUB_4u64(product_high, t_high);

            product_low = #VMOVSHDUP_256(product_low);
            product_low = #VPBLEND_8u32(product_low, product_high, 0b1_0_1_0_1_0_1_0);
            product.[u256 offset] = product_low;

            offset += 32;
        }

        return product;
    }

    fn conditionally_add_modulus(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial)
        -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        reg u256 zero mask;
        reg u256 lhs rhs;
        reg u256 modulus;
        reg u64 offset;

        modulus = MODULUS_VECTOR;

        ?{}, zero = #set0_256();

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            lhs = polynomial.[u256 offset];

            mask = #VPCMPGT_8u32(zero, lhs);
            rhs = #VPAND_256(modulus, mask);

            lhs = #VPADD_8u32(lhs, rhs);
            polynomial.[u256 offset] = lhs;

            offset += 32;
        }

        return polynomial;
    }

    inline
    fn check_infinity_norm(
        reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial,
        inline int threshold
    ) -> reg u8
    {
        reg u256 coefficients;
        reg u256 zero threshold_vector;
        reg u256 exceeds exceeds_negative exceeds_any;
        reg u64 msb_mask;
        reg u8 result;
        reg bool zf;

        reg u64 offset temp;

        temp = threshold - 1;
        threshold_vector = (256u)#VMOV(temp);
        threshold_vector = #VPBROADCAST_8u32(threshold_vector);

        ?{}, zero = #set0_256();
        ?{}, exceeds_any = #set0_256();

        result = 0;
        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            coefficients = polynomial.[u256 offset];

            // TODO: If performance is lacking, add and use mm256_abs_epi32()
            // instead of the following 3 lines.
            exceeds = #VPCMPGT_8u32(coefficients, threshold_vector);
            coefficients = #VPSUB_8u32(zero, coefficients);
            exceeds_negative = #VPCMPGT_8u32(coefficients, threshold_vector);

            exceeds = #VPOR_256(exceeds, exceeds_negative);
            exceeds_any = #VPOR_256(exceeds_any, exceeds);

            offset += 32;
        }

        msb_mask = #VPMOVMSKB_u256u64(exceeds_any);
        _, _, _, _, zf = #TEST_64(msb_mask, msb_mask);
        result = #SETcc(!zf);

        return result;
    }

    inline
    fn make_hint(
        reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] f0 f1,
        #spill_to_mmx reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] hints
    ) -> reg u32, reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        // TODO: To make this avx2, we'll need implementations of:
        // - mm256_abs_epi32()
        // - mm256_sign_epi32()
        reg u32 weight a0 a1 hint;
        #spill_to_mmx reg u64 i;

        reg u64 msf = #init_msf();

        inline bool condition;

        weight = 0;
        i = 0;
        while {condition = i < COEFFICIENTS_IN_POLYNOMIAL;} (condition) {
            msf = #update_msf(condition, msf);

            a0 = f0[i];
            a0 = #protect_32(a0, msf);

            a1 = f1[i];
            a1 = #protect_32(a1, msf);

            hint, msf = coefficient::make_hint(a0, a1, msf);

            hints[i] = hint;
            weight += hint;
            i += 1;
        }

        return weight, hints;
    }

    fn reduce32(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial)
        -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL]
    {
        reg u256 two_pow_22 modulus;
        reg u256 a t;

        reg u64 offset;

        two_pow_22 = #VMOVDQU_256(TWO_POW_22_VECTOR);
        modulus = #VMOVDQU_256(MODULUS_VECTOR);

        offset = 0;
        while(offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            // t = (a + (1 << 22)) >> 23
            a = polynomial.[u256 offset];
            t = #VPADD_8u32(a, two_pow_22);
            t = #VPSRA_8u32(t, 23);

            // t = a - t*Q
            t = #VPMULL_8u32(t, modulus);
            a = #VPSUB_8u32(a, t);

            polynomial.[u256 offset] = a;

            offset += 32;
        }

        return polynomial;
    }

    inline
    fn shift_coefficients_left(reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] polynomial) -> reg ptr u32[COEFFICIENTS_IN_POLYNOMIAL] {
        reg u256 coefficients;

        reg u64 offset;

        offset = 0;
        while (offset < COEFFICIENTS_IN_POLYNOMIAL * 4) {
            coefficients = polynomial.[u256 offset];
            coefficients = #VPSLL_8u32(coefficients, BITS_IN_LOWER_PART_OF_T);

            polynomial.[u256 offset] = coefficients;

            offset += 32;
        }

        return polynomial;
    }
}
